---
title: "Project PTDS"
author: "Luca Bron, David Germano, Patrik Grandadam, Vincent Lomazzi, Edgar Raisin"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    theme: cerulean
    highlight: tango
---

```{r}
library(robotstxt)
library(xml2)
library(rvest)
library(magrittr)
library(stringr)
```


# Webscrapping

Example data structure de Iegor a rearranger

```{r}
cities <- list(
  zurich = list(),
  geneve = list(),
  basel = list(),
  lausanne = list(),
  bern = list(),
  winterthur = list(),
  lucerne = list(),
  lugano = list()
  )
# 
# list(
#    zurich = list(page1 = c(12, 12, 1213),
#      page2 = c(1222, 12, 1213)),
#    geneva = list()
# )
# 
# 
#  # lausanne
# page1 <- c(12, 13, 14)
# attr(x = page1, which = "url") <- "http..."
# 
# 
# page2 <- c(11, 13, 14)
# attr(x = prices$zurich$page2, which = "url") <- "http...2"
# 
# prices <- list(
#   lausanne = list(page1, page2),
#   basel = list(page1, page2)
# )
# 
# prices
```


Cities of interest.

```{r}
# cities2 <- c("Zurich", "Geneve", "Basel", "Lausanne", "Bern",
#             "Winterthur", "Lucerne", "Lugano")
```


## Immoscout

Is the path allowed for each city? 

```{r, eval = FALSE}
paths_allowed(paste(paths="/en/real-estate/rent/city/", cities, sep=""),
              domain="https://www.immoscout24.ch")
get_robotstxt(domain="https://www.immoscout24.ch")
```
--> Yes 


LOOPING OVER EACH CITY

Getting the URL of each cities in new variables linked to each of them:

```{r}
# for (i in 1:length(cities2)){
# assign(paste("url_immoscout_", cities2[i], sep=""), paste("https://www.immoscout24.ch/en/real-estate/rent/city-", cities2[i], sep=""))
#   }
```

```{r}
for (i in 1:length(cities)){
  attr(cities[[i]], which="url") <- paste("https://www.immoscout24.ch/en/real-estate/rent/city-", names(cities[i]), sep="") 
}
```



```{r}
pages <- list()
for( i in 1:length(cities)){
  pages[[i]] <- read_html(x = paste0(unlist(attributes(cities[[i]]),use.names=FALSE))) %>%
  html_nodes(css = ".cXTlXt") %>%
  html_text() %>%
  as.numeric() %>%
  max(na.rm = TRUE) %>%
  subtract(e2 = 1) %>%
  seq(from = 1) 
}
```

Looping over cities and pages to get all prices for all cities
```{r}
# for (i in 1:length(cities2)){
#   for (page in paste("pages_immoscout_", cities2[i], sep="") %>% get){
#     url_path_page_immoscout <-
#       paste("https://www.immoscout24.ch/en/real-estate/rent/city-",
#       cities2[i], sep="") %>% 
#     paste("?pn=", page, sep="")
# 
# ## doesnt work...
# assign(paste("prices_immoscout_", cities2[i], sep="") %>% paste("_page_", page, sep=""), read_html(url_path_page_immoscout) %>% 
#   html_nodes(".fFoWRd") %>%
#   html_text())
#   }
# }
```

Getting the prices with the "Selector Gadget":

```{r}
# .fFoWRd --> the prices
# .kEhyoM .ehLwuE --> the nb of rooms and the surface
# .gDVamO .fUZwLT --> the address
# .gDVamO .fUZwLT , .kEhyoM .ehLwuE, .fFoWRd --> all combined without other perturbating lements
# .csgoYM --> the big box
```

Scrapping everything at once:
```{r}
for (i in 1:length(cities)){
  for (page in pages[[i]]){  
    url_path_page_immoscout <- cities[[i]] %>% 
      attributes %>% 
      unlist(use.names = FALSE) %>% 
      paste0 %>%
      paste("?pn=", page, sep="") 
      
    cities[[i]][[page]] <- list()
     cities[[i]][[page]] <- read_html(url_path_page_immoscout) %>% 
      html_nodes(".dAzzeo") %>%
      html_text()
  }
}

## Code to test uf working on the last page of Luganoc(a)
a <- read_html(url_path_page_immoscout) %>% 
      html_nodes(".csgoYM") %>%
      html_text() %>%
      gsub("\u00B2","",.) %>% 
       gsub(".\u2014","",.) %>% data.frame


unlisted_cities <- unlist(cities) %>% data.frame
zurich <- unlisted_cities[grep('^zurich', rownames(unlisted_cities)),] %>% data.frame
```

Iegor's code
```{r}
item_full_info <- read_html("https://www.immoscout24.ch/en/real-estate/rent/city-zurich?pn=5") %>% 
  html_nodes(".csgoYM") %>%
  html_text()

#extract number of rooms
#regexec(pattern = "rooms*", text = item_full_info)
str_extract(item_full_info, ".*rooms*") %>% gsub(pattern = " rooms", replacement = "", fixed = TRUE)

#extract Size
str_extract(item_full_info, ".*m\u00B2") %>% str_extract(., "rooms, .*") %>%
  gsub(pattern=" m\u00B2", replacement = "", fixed = TRUE) %>%
  gsub(pattern="rooms, ", replacement = "", fixed = TRUE)

#extract localisation
str_extract(item_full_info, ".*,") %>%  
  gsub(pattern = "1,", replacement = "", fixed = TRUE) %>%
  str_extract(., ".*,") %>%
  gsub(pattern = ",", replacement = "", fixed = TRUE) 
# %>%
#   gsub("\\s*\\w*$", replacement = "")

# mylist <- list()
# for (i in 1:length(item_full_info)){
#   if(str_extract(item_full_info, "eCHF .*")[i] %>% str_extract(., ".*.\u2014 *") %>% 
#      gsub(pattern = "eCHF ", replacement = "", fixed = TRUE) %>%
#      gsub(pattern = ".\u2014", replacement = "", fixed = TRUE) %>%
#      gsub(pattern = ",", replacement = "", fixed = TRUE ) > 1000
#      ){
#     mylist[i] <- returnValue(str_split(item_full_info, pattern=",")[[i]][length(str_split(item_full_info, pattern=",")[[i]])-2]) 
#   } else {mylist[i] <- returnValue(str_split(item_full_info, pattern=",")[[i]][length(str_split(item_full_info, pattern=",")[[i]])-1])
#   }
# }

# ca ca devrait marcher
str_extract(item_full_info, ".*,") %>% str_extract(., "\u00bb.*") %>% str_extract(., ".*Close") %>%
  gsub(pattern = "Close", replacement = "", fixed = TRUE) %>%
  gsub(pattern = ",", replacement = "", fixed = TRUE) %>%
  gsub(pattern = "\u00bb", replacement = "", fixed = TRUE)


#il faudrait faire des règles qui prennent les caractères à droite du chevron 
# str_split(item_full_info, pattern=",")[[i]][length(str_split(item_full_info, pattern=",")[[i]])-2]

#extract price
str_extract(item_full_info, "eCHF .*") %>% str_extract(., ".*.\u2014 *") %>% 
  gsub(pattern = "eCHF ", replacement = "", fixed = TRUE) %>%
  gsub(pattern = ".\u2014", replacement = "", fixed = TRUE) %>%
  gsub(pattern = ",", replacement = "", fixed = TRUE)
  #writting "eCHF" deal better with anomalies even though it's a bit barbaric
```

